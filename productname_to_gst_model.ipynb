{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWSQ+ut7DLttahJTBuG4PL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wardayX/cyhack/blob/main/productname_to_gst_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Imports  \n",
        "Install and import all required libraries.\n"
      ],
      "metadata": {
        "id": "neL9KG6bcjUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xavitI9Nb0w-",
        "outputId": "21d83f72-bb0c-49b4-f8fb-745de7350ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas pdfplumber fuzzywuzzy python-Levenshtein sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "from fuzzywuzzy import process as fuzzy_process\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import re\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"Libraries installed and imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Upload Functions  \n",
        "Define helper functions to upload the HSN PDF and GST CSV."
      ],
      "metadata": {
        "id": "ODQTkqAQcrkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_hsn_pdf():\n",
        "    print(\"Please upload your HSN Code PDF (SL NO, HS CODE, DESCRIPTION columns).\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return None, None\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    print(f\"Uploaded '{file_name}'\")\n",
        "    return file_name, uploaded[file_name]\n",
        "\n",
        "def upload_gst_csv():\n",
        "    print(\"\\nPlease upload your GST Rates CSV.\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return None, None\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    print(f\"Uploaded '{file_name}'\")\n",
        "    return file_name, uploaded[file_name]\n",
        "\n",
        "print(\"File upload functions defined.\")"
      ],
      "metadata": {
        "id": "itfhvLvwdVIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse HSN PDF with Aggregate Hierarchical Descriptions\n",
        "Extract HS codes and descriptions from the uploaded PDF with enriching parent HSN codes by concatenating child descriptions.\n"
      ],
      "metadata": {
        "id": "IGlTIflVdWTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_hsn_pdf(pdf_content):\n",
        "    data = []\n",
        "    try:\n",
        "        with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                print(f\"Processing PDF page {i+1}/{len(pdf.pages)}...\")\n",
        "                tables = page.extract_tables()\n",
        "                if tables:\n",
        "                    for table in tables:\n",
        "                        header = table[0]\n",
        "                        if header and 'HS CODE' in str(header).upper() and 'DESCRIPTION' in str(header).upper():\n",
        "                            data_rows = table[1:]\n",
        "                        else:\n",
        "                            data_rows = table\n",
        "\n",
        "                        for row in data_rows:\n",
        "                            if len(row) >= 3:\n",
        "                                sl_no, hs_code, description = row[0], row[1], row[2]\n",
        "                                hs_code = str(hs_code).replace('\\n', ' ').strip() if hs_code else None\n",
        "                                description = str(description).replace('\\n', ' ').strip() if description else None\n",
        "                                if hs_code and description:\n",
        "                                    data.append({'HS_Code_PDF': hs_code, 'Description_PDF': description})\n",
        "                    continue\n",
        "\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    lines = text.split('\\n')\n",
        "                    for line in lines:\n",
        "                        match_hs = re.search(r'^\\s*(\\d{4,8})\\s+(.+)', line)\n",
        "                        if match_hs:\n",
        "                            hs_code = match_hs.group(1).strip()\n",
        "                            description = match_hs.group(2).strip()\n",
        "                            description = re.sub(r'\\s{2,}', ' ', description)\n",
        "                            if hs_code and description:\n",
        "                                data.append({'HS_Code_PDF': hs_code, 'Description_PDF': description})\n",
        "                        else:\n",
        "                            print(f\"Could not parse line: {line}\")\n",
        "\n",
        "\n",
        "        if not data:\n",
        "            print(\"Warning: No data extracted from PDF. PDF parsing might need custom logic for your file format.\")\n",
        "            print(\"Consider using page.extract_text() and custom regex if tables are not well-structured.\")\n",
        "            return pd.DataFrame(columns=['HS_Code_PDF', 'Description_PDF'])\n",
        "\n",
        "        df_hsn = pd.DataFrame(data)\n",
        "        df_hsn['HS_Code_PDF'] = df_hsn['HS_Code_PDF'].astype(str).str.replace(r'\\W+', '', regex=True).str.strip().str.lower()\n",
        "        df_hsn.dropna(subset=['HS_Code_PDF'], inplace=True)\n",
        "        df_hsn.drop_duplicates(subset=['HS_Code_PDF'], keep='first', inplace=True)\n",
        "        print(f\"Extracted {len(df_hsn)} unique HSN entries from PDF.\")\n",
        "        return df_hsn\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing PDF: {e}\")\n",
        "        print(\"Please ensure the PDF is not scanned (image-based) and has extractable text.\")\n",
        "        return pd.DataFrame(columns=['HS_Code_PDF', 'Description_PDF'])\n",
        "\n",
        "print(\"PDF parsing function defined.\")\n",
        "\n",
        "def aggregate_hsn_descriptions(df_hsn_input):\n",
        "    if df_hsn_input.empty:\n",
        "        print(\"HSN PDF data is empty. Skipping aggregation.\")\n",
        "        return df_hsn_input.copy()\n",
        "\n",
        "    print(\"\\nAggregating hierarchical HSN descriptions from PDF data...\")\n",
        "    df_hsn = df_hsn_input.copy()\n",
        "    df_hsn['HS_Code_PDF'] = df_hsn['HS_Code_PDF'].astype(str)\n",
        "    df_hsn.sort_values(by='HS_Code_PDF', inplace=True)\n",
        "    df_hsn.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    aggregated_descriptions = {}\n",
        "    unique_hs_codes = sorted(df_hsn['HS_Code_PDF'].unique())\n",
        "\n",
        "    for parent_hs in unique_hs_codes:\n",
        "        parent_row = df_hsn[df_hsn['HS_Code_PDF'] == parent_hs]\n",
        "        if parent_row.empty or pd.isna(parent_row['Description_PDF'].iloc[0]):\n",
        "            continue\n",
        "\n",
        "        current_descriptions = [parent_row['Description_PDF'].iloc[0]]\n",
        "        for child_hs in unique_hs_codes:\n",
        "            if child_hs.startswith(parent_hs) and len(child_hs) > len(parent_hs):\n",
        "                child_row = df_hsn[df_hsn['HS_Code_PDF'] == child_hs]\n",
        "                if not child_row.empty and pd.notna(child_row['Description_PDF'].iloc[0]):\n",
        "                    current_descriptions.append(child_row['Description_PDF'].iloc[0])\n",
        "        aggregated_descriptions[parent_hs] = \". \".join(list(dict.fromkeys(current_descriptions)))\n",
        "\n",
        "    df_hsn['Aggregated_Description_PDF'] = df_hsn['HS_Code_PDF'].map(aggregated_descriptions)\n",
        "    df_hsn['Aggregated_Description_PDF'].fillna(df_hsn['Description_PDF'], inplace=True)\n",
        "\n",
        "    print(\"HSN description aggregation complete.\")\n",
        "    return df_hsn"
      ],
      "metadata": {
        "id": "7rLGOD5mdaBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse GST Rates CSV  \n",
        "Read the GST CSV with fallback encodings, clean and normalize columns.\n"
      ],
      "metadata": {
        "id": "0lmsqdLues9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_gst_csv(csv_content):\n",
        "    df_gst = None\n",
        "    encodings_to_try = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
        "\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            csv_file_like_object = io.BytesIO(csv_content)\n",
        "            df_gst = pd.read_csv(csv_file_like_object, encoding=encoding)\n",
        "            print(f\"GST CSV loaded successfully with encoding: {encoding}\")\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to decode CSV with encoding: {encoding}. Trying next...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV with encoding {encoding}: {e}\")\n",
        "            df_gst = None\n",
        "\n",
        "    if df_gst is None:\n",
        "        print(\"Error: Could not read or decode the GST CSV file with common encodings.\")\n",
        "        print(\"Please ensure your CSV file is saved in a compatible format (e.g., UTF-8, Latin-1, CP1252) or specify the correct encoding.\")\n",
        "        return pd.DataFrame()\n",
        "    print(\"GST CSV loaded. Original columns found:\", df_gst.columns.tolist())\n",
        "\n",
        "    column_mapping = {\n",
        "        'Chapter/Heading/Sub-heading/Tariffitem': 'HS_Code_GST',\n",
        "        'DescriptionofGoods': 'Description_GST',\n",
        "        'CGST(%)': 'CGST_Rate',\n",
        "        'SGST/UTGST(%)': 'SGST_Rate',\n",
        "        'IGST(%)': 'IGST_Rate',\n",
        "        'CompensationCess': 'Compensation_Cess_Raw'\n",
        "    }\n",
        "\n",
        "    actual_mapping_to_rename = {}\n",
        "    for csv_header_pattern, internal_name in column_mapping.items():\n",
        "        for actual_csv_column_name in df_gst.columns:\n",
        "            if csv_header_pattern.lower().strip() == actual_csv_column_name.lower().strip():\n",
        "                actual_mapping_to_rename[actual_csv_column_name] = internal_name\n",
        "                break\n",
        "\n",
        "    if actual_mapping_to_rename:\n",
        "        df_gst.rename(columns=actual_mapping_to_rename, inplace=True)\n",
        "        print(\"Columns after initial renaming attempt:\", df_gst.columns.tolist())\n",
        "    else:\n",
        "        print(\"No column renaming mappings were applied based on column_mapping.\")\n",
        "\n",
        "    required_internal_cols = ['HS_Code_GST', 'Description_GST', 'CGST_Rate', 'SGST_Rate', 'IGST_Rate']\n",
        "\n",
        "    if 'Compensation_Cess_Raw' not in df_gst.columns:\n",
        "        found_cess_col = None\n",
        "        for col_name in df_gst.columns:\n",
        "            if \"compensationcess\" == col_name.lower().strip() and col_name not in actual_mapping_to_rename.values():\n",
        "                found_cess_col = col_name\n",
        "                break\n",
        "            elif \"cess\" in col_name.lower() and \"compensation\" in col_name.lower() and col_name not in actual_mapping_to_rename.values():\n",
        "                found_cess_col = col_name\n",
        "                break\n",
        "        if found_cess_col:\n",
        "            df_gst.rename(columns={found_cess_col: 'Compensation_Cess_Raw'}, inplace=True)\n",
        "            print(f\"Renamed '{found_cess_col}' to 'Compensation_Cess_Raw'.\")\n",
        "        else:\n",
        "            print(\"Warning: 'Compensation_Cess_Raw' (or a mappable equivalent) not found. Assuming no compensation cess for now.\")\n",
        "            df_gst['Compensation_Cess_Raw'] = \"Nil\"\n",
        "\n",
        "    required_internal_cols.append('Compensation_Cess_Raw')\n",
        "\n",
        "    missing_cols = [col for col in required_internal_cols if col not in df_gst.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Error: Missing expected internal columns after all renaming attempts: {missing_cols}\")\n",
        "        print(f\"Current available columns in DataFrame: {df_gst.columns.tolist()}\")\n",
        "        print(\"Please ensure your `column_mapping` in Cell 4 correctly maps your CSV headers to the expected internal names.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_gst = df_gst[required_internal_cols].copy()\n",
        "    df_gst['HS_Code_GST'] = df_gst['HS_Code_GST'].astype(str).str.replace(r'\\W+', '', regex=True).str.strip().str.lower()\n",
        "    rate_cols_internal = ['CGST_Rate', 'SGST_Rate', 'IGST_Rate']\n",
        "    for col in rate_cols_internal:\n",
        "        df_gst[col] = df_gst[col].astype(str).str.replace('%', '').str.strip()\n",
        "        df_gst[col] = pd.to_numeric(df_gst[col], errors='coerce').fillna(0)\n",
        "\n",
        "    def parse_cess(value):\n",
        "        value_str = str(value).lower().strip()\n",
        "        if not value_str or value_str in ['no', 'false', 'nil', 'exempt', 'exempted', '0', '0%', '0.0', '0.0%','-']:\n",
        "            return 0.0, False\n",
        "        match_rate = re.search(r'(\\d+\\.?\\d*)', value_str)\n",
        "        if match_rate:\n",
        "            try:\n",
        "                rate = float(match_rate.group(1))\n",
        "                return rate, True\n",
        "            except ValueError:\n",
        "                pass\n",
        "        if pd.notna(value) and value_str not in ['no', 'false', 'nil', 'exempt', 'exempted', '0', '0%', '0.0', '0.0%','-']:\n",
        "            return 0.0, True\n",
        "        return 0.0, False\n",
        "\n",
        "    cess_parsed = df_gst['Compensation_Cess_Raw'].apply(parse_cess)\n",
        "    df_gst['Compensation_Cess_Rate'] = cess_parsed.apply(lambda x: x[0])\n",
        "    df_gst['Is_Compensation_Cess'] = cess_parsed.apply(lambda x: x[1])\n",
        "    df_gst.drop(columns=['Compensation_Cess_Raw'], inplace=True)\n",
        "\n",
        "    df_gst['Is_Exempted'] = (\n",
        "        (df_gst['CGST_Rate'] == 0) &\n",
        "        (df_gst['SGST_Rate'] == 0) &\n",
        "        (df_gst['IGST_Rate'] == 0) &\n",
        "        ((~df_gst['Is_Compensation_Cess']) | (df_gst['Compensation_Cess_Rate'] == 0))\n",
        "    )\n",
        "\n",
        "    df_gst.dropna(subset=['HS_Code_GST'], inplace=True)\n",
        "    df_gst.drop_duplicates(subset=['HS_Code_GST'], keep='first', inplace=True)\n",
        "    print(f\"Processed {len(df_gst)} unique HSN entries from GST CSV.\")\n",
        "    print(\"Final columns in GST DataFrame after processing:\", df_gst.columns.tolist())\n",
        "    return df_gst\n",
        "\n",
        "print(\"GST CSV parsing function (CORRECTED RENAMING LOGIC + ENCODING HANDLING) defined.\")"
      ],
      "metadata": {
        "id": "7-mub0XAessp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Parse Inputs  \n",
        "1. Upload HSN PDF → parse → aggregate  \n",
        "2. Upload GST CSV → parse  \n"
      ],
      "metadata": {
        "id": "TTcFN39PfP27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hsn_pdf_name, hsn_pdf_content = upload_hsn_pdf()\n",
        "df_hsn_pdf = pd.DataFrame()\n",
        "if hsn_pdf_content:\n",
        "    df_hsn_pdf = parse_hsn_pdf(hsn_pdf_content)\n",
        "    if not df_hsn_pdf.empty:\n",
        "        print(\"\\n--- Parsed HSN PDF Data (Sample BEFORE Aggregation) ---\")\n",
        "        print(df_hsn_pdf.head())\n",
        "        df_hsn_pdf_aggregated = aggregate_hsn_descriptions(df_hsn_pdf)\n",
        "    else:\n",
        "        df_hsn_pdf_aggregated = pd.DataFrame()\n",
        "else:\n",
        "    df_hsn_pdf_aggregated = pd.DataFrame()\n",
        "    print(\"\\nHSN PDF content not available. Skipping HSN PDF processing and aggregation.\")\n",
        "\n",
        "gst_csv_name, gst_csv_content = upload_gst_csv()\n",
        "df_gst_rates = pd.DataFrame()\n",
        "if gst_csv_content:\n",
        "    df_gst_rates = parse_gst_csv(gst_csv_content)\n",
        "\n",
        "gst_csv_name, gst_csv_content = upload_gst_csv()\n",
        "df_gst_rates = pd.DataFrame()\n",
        "if gst_csv_content:\n",
        "    df_gst_rates = parse_gst_csv(gst_csv_content)\n",
        "    if not df_gst_rates.empty:\n",
        "        print(\"\\n--- Parsed GST Rates CSV Data (Sample) ---\")\n",
        "        print(df_gst_rates.head())\n",
        "        print(\"\\nGST Data Columns:\", df_gst_rates.columns)\n",
        "\n",
        "if df_hsn_pdf.empty and df_gst_rates.empty:\n",
        "    print(\"\\nERROR: Neither HSN PDF nor GST CSV data could be loaded. Cannot proceed.\")\n",
        "elif df_gst_rates.empty:\n",
        "    print(\"\\nERROR: GST CSV data could not be loaded. Tax rates are essential. Cannot proceed.\")"
      ],
      "metadata": {
        "id": "tYSZS_Xgfgdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge HSN & GST Data  \n",
        "Left-join on HS codes, prioritize aggregated descriptions.\n"
      ],
      "metadata": {
        "id": "4RiR9BC-fvqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.DataFrame()\n",
        "\n",
        "hsn_data_to_merge = (\n",
        "    df_hsn_pdf_aggregated\n",
        "    if 'df_hsn_pdf_aggregated' in locals() and not df_hsn_pdf_aggregated.empty\n",
        "    else df_hsn_pdf\n",
        ")\n",
        "\n",
        "if not df_gst_rates.empty:\n",
        "    if not hsn_data_to_merge.empty:\n",
        "        df_merged = pd.merge(\n",
        "            df_gst_rates,\n",
        "            hsn_data_to_merge,\n",
        "            left_on='HS_Code_GST',\n",
        "            right_on='HS_Code_PDF',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        if 'Aggregated_Description_PDF' in df_merged.columns:\n",
        "            df_merged['Description_From_PDF_Source'] = df_merged['Aggregated_Description_PDF']\n",
        "        else:\n",
        "            df_merged['Description_From_PDF_Source'] = df_merged['Description_PDF']\n",
        "\n",
        "        df_merged['Combined_Description'] = df_merged['Description_From_PDF_Source'].fillna(df_merged['Description_GST'])\n",
        "        df_merged['Combined_Description'].fillna('', inplace=True)\n",
        "\n",
        "        df_merged.rename(columns={'HS_Code_GST': 'HS_Code'}, inplace=True)\n",
        "\n",
        "        final_columns = [\n",
        "            'HS_Code',\n",
        "            'Combined_Description',\n",
        "            'Description_GST',\n",
        "            'Description_PDF',\n",
        "            'Aggregated_Description_PDF',\n",
        "            'CGST_Rate',\n",
        "            'SGST_Rate',\n",
        "            'IGST_Rate',\n",
        "            'Is_Compensation_Cess',\n",
        "            'Compensation_Cess_Rate',\n",
        "            'Is_Exempted'\n",
        "        ]\n",
        "        final_columns = [col for col in final_columns if col in df_merged.columns]\n",
        "        df_merged = df_merged[final_columns]\n",
        "\n",
        "        print(df_merged.head())\n",
        "\n",
        "    else:\n",
        "        df_merged = df_gst_rates.copy()\n",
        "        df_merged.rename(\n",
        "            columns={\n",
        "                'HS_Code_GST': 'HS_Code',\n",
        "                'Description_GST': 'Combined_Description'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        if 'Description_PDF' not in df_merged.columns:\n",
        "            df_merged['Description_PDF'] = None\n",
        "        if 'Aggregated_Description_PDF' not in df_merged.columns:\n",
        "            df_merged['Aggregated_Description_PDF'] = None\n",
        "        if 'Is_Exempted' not in df_merged.columns:\n",
        "            df_merged['Is_Exempted'] = (\n",
        "                (df_merged['CGST_Rate'] == 0) &\n",
        "                (df_merged['SGST_Rate'] == 0) &\n",
        "                (df_merged['IGST_Rate'] == 0) &\n",
        "                (~df_merged['Is_Compensation_Cess'] | (df_merged['Compensation_Cess_Rate'] == 0))\n",
        "            )\n",
        "\n",
        "    if 'Combined_Description' in df_merged.columns:\n",
        "        df_merged['Combined_Description'] = (\n",
        "            df_merged['Combined_Description']\n",
        "            .astype(str)\n",
        "            .str.lower()\n",
        "            .str.strip()\n",
        "        )\n",
        "    else:\n",
        "        df_merged['Combined_Description'] = \"\"\n",
        "\n",
        "    if not df_merged.empty:\n",
        "        print(f\"Final merged dataset has {len(df_merged)} entries.\")\n",
        "        print(\"Columns in final merged data:\", df_merged.columns.tolist())\n",
        "        if 'Combined_Description' in df_merged.columns:\n",
        "            all_descriptions_for_fuzzy = (\n",
        "                df_merged['Combined_Description']\n",
        "                .fillna('')\n",
        "                .astype(str)\n",
        "                .unique()\n",
        "                .tolist()\n",
        "            )\n",
        "            print(f\"Created 'all_descriptions_for_fuzzy' with {len(all_descriptions_for_fuzzy)} items.\")\n",
        "        else:\n",
        "            all_descriptions_for_fuzzy = []\n",
        "    else:\n",
        "        all_descriptions_for_fuzzy = []\n",
        "        print(\"ERROR: Merged data is empty.\")\n",
        "else:\n",
        "    print(\"Essential GST rate data is missing. Cannot create merged dataset.\")\n"
      ],
      "metadata": {
        "id": "M2O7hwD5gRFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Search Initialization  \n",
        "Load Sentence-Transformer model and compute embeddings.\n"
      ],
      "metadata": {
        "id": "Q195zIblgrQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "corpus_embeddings = None\n",
        "\n",
        "if 'df_merged' in locals() and not df_merged.empty and \\\n",
        "   'Combined_Description' in df_merged.columns and not df_merged['Combined_Description'].dropna().empty:\n",
        "    try:\n",
        "        print(\"\\nLoading sentence transformer model for semantic search...\")\n",
        "        model_name = 'all-mpnet-base-v2'\n",
        "\n",
        "        model = SentenceTransformer(model_name)\n",
        "        print(f\"Model '{model_name}' loaded.\")\n",
        "\n",
        "        print(\"Computing embeddings for product descriptions...\")\n",
        "        descriptions_to_embed = df_merged['Combined_Description'].fillna('').astype(str).tolist()\n",
        "\n",
        "        if descriptions_to_embed:\n",
        "            corpus_embeddings = model.encode(descriptions_to_embed, convert_to_tensor=True, show_progress_bar=True)\n",
        "            print(\"Embeddings computed.\")\n",
        "        else:\n",
        "            print(\"No descriptions to embed.\")\n",
        "            corpus_embeddings = None\n",
        "            model = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing model or computing embeddings: {e}\")\n",
        "        model = None\n",
        "        corpus_embeddings = None\n",
        "else:\n",
        "    print(\"\\nSkipping semantic search setup.\")\n",
        "    model = None\n",
        "    corpus_embeddings = None\n"
      ],
      "metadata": {
        "id": "O_cDypjDg5bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Setup & QA Pipeline Setup  \n"
      ],
      "metadata": {
        "id": "ZPvLl6N0hvdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "tfidf_vectorizer = None\n",
        "tfidf_matrix = None\n",
        "tfidf_feature_names = None\n",
        "\n",
        "if 'df_merged' in locals() and not df_merged.empty and \\\n",
        "   'Combined_Description' in df_merged.columns and not df_merged['Combined_Description'].dropna().empty:\n",
        "    try:\n",
        "        print(\"\\nSetting up TF-IDF Vectorizer...\")\n",
        "        corpus_for_tfidf = df_merged['Combined_Description'].fillna('').astype(str).tolist()\n",
        "\n",
        "        if corpus_for_tfidf:\n",
        "            tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "            tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_for_tfidf)\n",
        "            tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "            print(f\"TF-IDF matrix computed. Shape: {tfidf_matrix.shape}\")\n",
        "            print(f\"Number of TF-IDF features: {len(tfidf_feature_names)}\")\n",
        "        else:\n",
        "            print(\"No descriptions available for TF-IDF setup.\")\n",
        "            tfidf_vectorizer = None\n",
        "            tfidf_matrix = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during TF-IDF setup: {e}\")\n",
        "        tfidf_vectorizer = None\n",
        "        tfidf_matrix = None\n",
        "else:\n",
        "    print(\"\\nSkipping TF-IDF setup.\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = None\n",
        "qa_model_name = 'deepset/roberta-base-squad2'\n",
        "\n",
        "try:\n",
        "    print(f\"\\nLoading QA pipeline with model: {qa_model_name}...\")\n",
        "    qa_pipeline = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\n",
        "    print(\"QA pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading QA pipeline: {e}\")\n"
      ],
      "metadata": {
        "id": "Xf1lRWgxhU9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search & Format Helpers  \n",
        "Functions for fuzzy, TF-IDF, semantic search, and result formatting.\n"
      ],
      "metadata": {
        "id": "M_uo8J-eh99G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_gst_by_description_tfidf(description_query, top_k=5, similarity_threshold=0.2):\n",
        "    if df_merged.empty or tfidf_vectorizer is None or tfidf_matrix is None or tfidf_matrix.shape[0] == 0:\n",
        "        print(\"TF-IDF not available or data empty.\")\n",
        "        return []\n",
        "\n",
        "    query_vector = tfidf_vectorizer.transform([str(description_query).lower().strip()])\n",
        "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    relevant_indices = np.where(cosine_similarities >= similarity_threshold)[0]\n",
        "    if len(relevant_indices) == 0:\n",
        "        return []\n",
        "\n",
        "    scores = cosine_similarities[relevant_indices]\n",
        "    sorted_indices = relevant_indices[np.argsort(scores)[::-1]][:top_k]\n",
        "\n",
        "    matched = []\n",
        "    seen = set()\n",
        "    for idx in sorted_indices:\n",
        "        row = df_merged.iloc[idx]\n",
        "        code = row['HS_Code']\n",
        "        if code not in seen:\n",
        "            matched.append({'row': row, 'score': cosine_similarities[idx] * 100, 'match_type': 'TF-IDF'})\n",
        "            seen.add(code)\n",
        "            if len(matched) == top_k:\n",
        "                break\n",
        "    return matched\n",
        "\n",
        "def format_gst_info(row):\n",
        "    if row is None or row.empty:\n",
        "        return \"No product information found.\"\n",
        "    hs = row.get('HS_Code', 'N/A')\n",
        "    desc = row.get('Combined_Description', 'N/A').capitalize()\n",
        "    cg = row.get('CGST_Rate', 0)\n",
        "    sg = row.get('SGST_Rate', 0)\n",
        "    ig = row.get('IGST_Rate', 0)\n",
        "    cess = row.get('Is_Compensation_Cess', False)\n",
        "    cr = row.get('Compensation_Cess_Rate', 0)\n",
        "    ex = row.get('Is_Exempted', False)\n",
        "\n",
        "    info = \"\\n--- Product GST Details ---\\n\"\n",
        "    info += f\"HS Code: {hs}\\nDescription: {desc}\\nCGST: {cg}%\\nSGST/UTGST: {sg}%\\nIGST: {ig}%\\n\"\n",
        "    info += f\"Compensation Cess Applicable: {'Yes' if cess else 'No'}\\n\"\n",
        "    if cess:\n",
        "        info += f\"Compensation Cess Rate: {cr}%\\n\"\n",
        "    info += f\"Exempted from Tax: {'Yes' if ex else 'No'}\\n\"\n",
        "    info += \"--------------------------\\n\"\n",
        "    return info\n",
        "\n",
        "def find_gst_by_hs_code(hs_code_query):\n",
        "    if df_merged.empty:\n",
        "        return []\n",
        "    query = str(hs_code_query).strip().lower()\n",
        "    return [row for _, row in df_merged[df_merged['HS_Code'] == query].iterrows()]\n",
        "\n",
        "def find_gst_by_description_fuzzy(description_query, threshold=80, limit=5):\n",
        "    if df_merged.empty or not all_descriptions_for_fuzzy:\n",
        "        return []\n",
        "    matches = fuzzy_process.extract(str(description_query).lower().strip(),\n",
        "                                    all_descriptions_for_fuzzy, limit=limit*2)\n",
        "    matched = []\n",
        "    seen = set()\n",
        "    for desc_match, score in matches:\n",
        "        if score >= threshold:\n",
        "            for _, row in df_merged[df_merged['Combined_Description'] == desc_match].iterrows():\n",
        "                code = row['HS_Code']\n",
        "                if code not in seen:\n",
        "                    matched.append({'row': row, 'score': score, 'match_type': 'fuzzy'})\n",
        "                    seen.add(code)\n",
        "                    if len(matched) == limit:\n",
        "                        break\n",
        "        if len(matched) == limit:\n",
        "            break\n",
        "    return sorted(matched, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "def find_gst_by_description_semantic(description_query, top_k=5, similarity_threshold=0.55):\n",
        "    if df_merged.empty or model is None or corpus_embeddings is None or corpus_embeddings.nelement() == 0:\n",
        "        return []\n",
        "    query_embedding = model.encode(str(description_query).lower().strip(), convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top = torch.topk(cos_scores, k=min(top_k*2, len(cos_scores)))\n",
        "    matched = []\n",
        "    seen = set()\n",
        "    for score, idx in zip(top.values.tolist(), top.indices.tolist()):\n",
        "        if score >= similarity_threshold:\n",
        "            row = df_merged.iloc[idx]\n",
        "            code = row['HS_Code']\n",
        "            if code not in seen:\n",
        "                matched.append({'row': row, 'score': score, 'match_type': 'semantic'})\n",
        "                seen.add(code)\n",
        "                if len(matched) == top_k:\n",
        "                    break\n",
        "        else:\n",
        "            break\n",
        "    return sorted(matched, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "print(\"Search functions defined.\")\n"
      ],
      "metadata": {
        "id": "0pTjqxowiBm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer QA System  \n",
        "Define the interactive QA loop.\n"
      ],
      "metadata": {
        "id": "mDD2IrB-i-Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_extracted_gst_details(answer_text, item_context_row=None):\n",
        "    details = f\"QA Model Answer: '{answer_text}'\\n\"\n",
        "    if item_context_row is not None:\n",
        "        hs = item_context_row.get('HS_Code', 'N/A')\n",
        "        desc = item_context_row.get('Combined_Description', 'N/A')\n",
        "        cg = item_context_row.get('CGST_Rate', 'N/A')\n",
        "        sg = item_context_row.get('SGST_Rate', 'N/A')\n",
        "        ig = item_context_row.get('IGST_Rate', 'N/A')\n",
        "        cess_app = item_context_row.get('Is_Compensation_Cess', False)\n",
        "        cr = item_context_row.get('Compensation_Cess_Rate', 'N/A')\n",
        "        ex = item_context_row.get('Is_Exempted', False)\n",
        "\n",
        "        details += f\"Based on context for HS Code {hs}: {desc}\\n\"\n",
        "        details += f\"  CGST: {cg}%, SGST: {sg}%, IGST: {ig}%\\n\"\n",
        "        details += f\"  Cess Applicable: {'Yes' if cess_app else 'No'}\\n\"\n",
        "        if cess_app:\n",
        "            details += f\"  Cess Rate: {cr}%\\n\"\n",
        "        details += f\"  Exempted: {'Yes' if ex else 'No'}\\n\"\n",
        "    return details\n",
        "\n",
        "\n",
        "def get_gst_with_transformer_qa():\n",
        "    if 'df_merged' not in globals() or df_merged.empty:\n",
        "        print(\"ERROR: df_merged is not available or empty.\"); return\n",
        "    if 'qa_pipeline' not in globals() or qa_pipeline is None:\n",
        "        print(\"ERROR: qa_pipeline is not available.\"); return\n",
        "    if 'model' not in globals() or model is None or \\\n",
        "       'corpus_embeddings' not in globals() or corpus_embeddings is None or corpus_embeddings.nelement() == 0:\n",
        "        print(\"ERROR: Semantic search components not available.\"); return\n",
        "\n",
        "    print(\"\\n--- GST QA with Transformer ---\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nEnter your product description or HS code: \").strip()\n",
        "        if user_query.lower() == 'exit':\n",
        "            print(\"Exiting QA system.\"); break\n",
        "        if not user_query:\n",
        "            continue\n",
        "\n",
        "        print(f\"Original query: '{user_query}'\")\n",
        "        embedding = model.encode(user_query, convert_to_tensor=True)\n",
        "        cos_scores = util.cos_sim(embedding, corpus_embeddings)[0]\n",
        "        top_results = torch.topk(cos_scores, k=min(3, len(df_merged)))\n",
        "\n",
        "        retrieved = []\n",
        "        for score, idx in zip(top_results.values.tolist(), top_results.indices.tolist()):\n",
        "            if score > 0.3:\n",
        "                retrieved.append({'row': df_merged.iloc[idx], 'score': score})\n",
        "\n",
        "        if not retrieved:\n",
        "            print(\"No relevant items found.\"); continue\n",
        "\n",
        "        print(f\"Found {len(retrieved)} relevant items.\")\n",
        "        questions = {\n",
        "            \"CGST\": \"What is the CGST rate?\",\n",
        "            \"SGST\": \"What is the SGST or UTGST rate?\",\n",
        "            \"IGST\": \"What is the IGST rate?\",\n",
        "            \"Cess_Applicable\": \"Is compensation cess applicable?\",\n",
        "            \"Cess_Rate\": \"What is the compensation cess rate?\",\n",
        "            \"Is_Exempted\": \"Is the product exempted from tax?\"\n",
        "        }\n",
        "\n",
        "        for item in retrieved[:1]:\n",
        "            row = item['row']\n",
        "            desc = row.get('Combined_Description', 'N/A')\n",
        "            hs = row.get('HS_Code', 'N/A')\n",
        "            context = (\n",
        "                f\"The product is '{desc}' with HS Code {hs}. \"\n",
        "                f\"CGST: {row.get('CGST_Rate', 'unknown')}%. \"\n",
        "                f\"SGST/UTGST: {row.get('SGST_Rate', 'unknown')}%. \"\n",
        "                f\"IGST: {row.get('IGST_Rate', 'unknown')}%. \"\n",
        "                f\"Compensation cess is {'applicable' if row.get('Is_Compensation_Cess') else 'not applicable'}. \"\n",
        "                f\"Cess rate: {row.get('Compensation_Cess_Rate', '0')}%. \"\n",
        "                f\"Exempted: {'yes' if row.get('Is_Exempted') else 'no'}.\"\n",
        "            )\n",
        "\n",
        "            print(f\"\\n--- Analyzing: {desc} (HS: {hs}) ---\")\n",
        "            print(f\"Retrieval Score: {item['score']:.2f}\")\n",
        "\n",
        "            details = {}\n",
        "            for key, q in questions.items():\n",
        "                try:\n",
        "                    res = qa_pipeline({'question': q, 'context': context})\n",
        "                    details[key] = {'answer': res['answer'], 'score': res['score']}\n",
        "                except:\n",
        "                    details[key] = {'answer': 'Error', 'score': 0.0}\n",
        "\n",
        "            print(f\"GST Details for HS {hs}:\")\n",
        "            print(f\"  CGST: {details['CGST']['answer']} (Conf: {details['CGST']['score']:.2f})\")\n",
        "            print(f\"  SGST/UTGST: {details['SGST']['answer']} (Conf: {details['SGST']['score']:.2f})\")\n",
        "            print(f\"  IGST: {details['IGST']['answer']} (Conf: {details['IGST']['score']:.2f})\")\n",
        "            print(f\"  Cess Applicable: {details['Cess_Applicable']['answer']} (Conf: {details['Cess_Applicable']['score']:.2f})\")\n",
        "            if 'applicable' in details['Cess_Applicable']['answer'].lower():\n",
        "                print(f\"  Cess Rate: {details['Cess_Rate']['answer']} (Conf: {details['Cess_Rate']['score']:.2f})\")\n",
        "            print(f\"  Exempted: {details['Is_Exempted']['answer']} (Conf: {details['Is_Exempted']['score']:.2f})\")\n",
        "            print(\"---\")\n"
      ],
      "metadata": {
        "id": "qKYGN7GwintU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-QA Sanity Check & Launch  \n",
        "Verify all components are ready and start the QA loop.\n"
      ],
      "metadata": {
        "id": "uRfM8292jSyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Pre-QA Sanity Check ---\")\n",
        "all_systems_go = True\n",
        "\n",
        "if 'df_merged' in globals() and not df_merged.empty:\n",
        "    print(f\"df_merged is READY for QA. Shape: {df_merged.shape}\")\n",
        "else:\n",
        "    print(\"CRITICAL: df_merged is empty or not defined.\")\n",
        "    if 'df_merged' in globals():\n",
        "        print(f\"df_merged.empty check result: {df_merged.empty}\")\n",
        "    all_systems_go = False\n",
        "\n",
        "if 'model' in globals() and model is not None and \\\n",
        "   'corpus_embeddings' in globals() and corpus_embeddings is not None and corpus_embeddings.nelement() > 0:\n",
        "    print(\"Semantic model and corpus_embeddings are READY for retrieval.\")\n",
        "else:\n",
        "    print(\"CRITICAL: Semantic model or corpus_embeddings are NOT ready.\")\n",
        "    if 'model' not in globals() or model is None:\n",
        "        print(\"model is not ready.\")\n",
        "    if 'corpus_embeddings' not in globals() or corpus_embeddings is None:\n",
        "        print(\"corpus_embeddings is not defined.\")\n",
        "    elif corpus_embeddings.nelement() == 0:\n",
        "        print(\"corpus_embeddings is empty.\")\n",
        "    all_systems_go = False\n",
        "\n",
        "if 'qa_pipeline' in globals() and qa_pipeline is not None:\n",
        "    print(\"QA pipeline is READY.\")\n",
        "else:\n",
        "    print(\"CRITICAL: QA pipeline is NOT ready.\")\n",
        "    all_systems_go = False\n",
        "\n",
        "print(\"--- End of Pre-QA Sanity Check ---\")\n",
        "\n",
        "if all_systems_go:\n",
        "    print(\"\\nAll checks passed. Starting QA system...\")\n",
        "    get_gst_with_transformer_qa()\n",
        "else:\n",
        "    print(\"\\nOne or more critical components are not ready. Cannot start QA system.\")\n",
        "    print(\"Please review the CRITICAL messages above.\")\n"
      ],
      "metadata": {
        "id": "SuapexKUjm0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}